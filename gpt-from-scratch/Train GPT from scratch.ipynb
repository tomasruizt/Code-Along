{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5976f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a3b952e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text: str = Path(\"tiny-shakespeare.txt\").read_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fa77d1",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f3750e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50364b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = dict(enumerate(chars))\n",
    "stoi = {v:k for k,v in itos.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "070555a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(s: str) -> list[int]:\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(ints: list[int]) -> str:\n",
    "    return \"\".join(itos[i] for i in ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf724fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"yay\" == decode(encode(\"yay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85ae0726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f0faae-11c2-4857-badb-4c3c453e244e",
   "metadata": {},
   "source": [
    "# Test / Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "095059a9-53cd-4470-b3e9-091c3b1adfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "test_data = data[n:]\n",
    "assert len(data) == len(train_data) + len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7264228e-eb30-409f-a043-adf63845d1cd",
   "metadata": {},
   "source": [
    "# Inputs and Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b310a40c-919e-4ce2-ab20-ab04f0e163c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8 # or context length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71380b79-811e-414e-9b63-7502282cb172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context=tensor([18]), target=47\n",
      "context=tensor([18, 47]), target=56\n",
      "context=tensor([18, 47, 56]), target=57\n",
      "context=tensor([18, 47, 56, 57]), target=58\n",
      "context=tensor([18, 47, 56, 57, 58]), target=1\n",
      "context=tensor([18, 47, 56, 57, 58,  1]), target=15\n",
      "context=tensor([18, 47, 56, 57, 58,  1, 15]), target=47\n",
      "context=tensor([18, 47, 56, 57, 58,  1, 15, 47]), target=58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"context={context}, target={target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164f93fd-eaeb-40f6-b6ff-f3a0899620f8",
   "metadata": {},
   "source": [
    "# Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e33a83fa-bf66-4bc8-a919-c6ae42bcb29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
       "         [44, 53, 56,  1, 58, 46, 39, 58],\n",
       "         [52, 58,  1, 58, 46, 39, 58,  1],\n",
       "         [25, 17, 27, 10,  0, 21,  1, 54]]),\n",
       " tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
       "         [53, 56,  1, 58, 46, 39, 58,  1],\n",
       "         [58,  1, 58, 46, 39, 58,  1, 46],\n",
       "         [17, 27, 10,  0, 21,  1, 54, 39]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "def get_batch(data: Tensor) -> Tensor:\n",
    "    start_ixs = torch.randint(len(data) - block_size, size=(batch_size,))\n",
    "    xs = torch.stack([data[i:i+block_size] for i in start_ixs])\n",
    "    ys = torch.stack([data[i+1:i+block_size+1] for i in start_ixs])\n",
    "    return xs, ys\n",
    "\n",
    "get_batch(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2145dfd-d768-4be9-bf43-6fbf5feef77a",
   "metadata": {},
   "source": [
    "# Efficient Self-Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31a68b7d-84f5-4f60-b09c-4b27a574dfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, head_size: int, dim_embedding: int):\n",
    "        super().__init__()\n",
    "        C = dim_embedding\n",
    "        self.key = nn.Linear(C, head_size, bias=False)\n",
    "        self.query = nn.Linear(C, head_size, bias=False)\n",
    "        self.value = nn.Linear(C, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, xs: Tensor) -> Tensor:\n",
    "        B,T,C = xs.shape\n",
    "        k = self.key(xs)   # (B, T, 16)\n",
    "        q = self.query(xs) # (B, T, 16)\n",
    "        ws =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "        ws = ws * C**-0.5 # scale\n",
    "        \n",
    "        ws = ws.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
    "        ws = F.softmax(ws, dim=-1)\n",
    "        \n",
    "        return ws @ self.value(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b633a0-8a8f-4ab6-8dc7-0e5bb8aef626",
   "metadata": {},
   "source": [
    "# GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15faa08e-a92a-44de-92eb-5be630275ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "class BGLM(nn.Module):\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super().__init__()\n",
    "        dim_embedding = 32 # C\n",
    "        self.embedding = nn.Embedding(vocab_size, dim_embedding)\n",
    "        self.pos_embedding = nn.Embedding(block_size, dim_embedding)\n",
    "        self.sa_head = SelfAttention(head_size=dim_embedding, dim_embedding=dim_embedding)\n",
    "        self.lm_head = nn.Linear(dim_embedding, vocab_size)\n",
    "\n",
    "    def forward(self, xs: Tensor, ys: Tensor = None):  # Both size (B,T)\n",
    "        B, T = xs.shape\n",
    "        token_emb = self.embedding(xs)  # size (B,T,C)\n",
    "        pos_emb = self.pos_embedding(torch.arange(T)) # (T,C)\n",
    "        x = token_emb + pos_emb # (B,T,C)\n",
    "        x = self.sa_head(x)\n",
    "        logits = self.lm_head(x) # (B,T,vocab)\n",
    "        if ys is None:\n",
    "            loss = None\n",
    "            return logits, loss\n",
    "        \n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        ys = ys.view(B*T)\n",
    "        loss = F.cross_entropy(logits, ys)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, xs: Tensor, n: int) -> Tensor:\n",
    "        \"\"\"Expands each x in xs to have 'n' more tokens\"\"\"\n",
    "        for _ in range(n):\n",
    "            xs_next = self.generate1(xs[:, -block_size:])\n",
    "            xs = torch.cat((xs, xs_next), dim=1) # (B,T+1)\n",
    "        return xs\n",
    "\n",
    "    def generate1(self, xs: Tensor) -> Tensor:\n",
    "        logits, _ = self(xs)\n",
    "        last_timestep = logits[:, -1, :]  # (B,C)\n",
    "        probs = F.softmax(last_timestep, dim=-1)  # (B,C)\n",
    "        xs_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "        return xs_next\n",
    "\n",
    "m = BGLM(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df75ddad-ca0f-4d71-b100-1571ef3a9692",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_, ys_ = get_batch(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9545a3e3-6b25-4951-9860-13b60a25039b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[41],\n",
       "        [41],\n",
       "        [42],\n",
       "        [50]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, loss = m(xs_, ys_)\n",
    "m.generate1(xs_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ef6dbe-ed73-47fa-8b60-e7d8d92860ee",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "As we see below, the model is still random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28c1588b-08b2-4c7e-bbd1-9a951e2a2462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\njVJDq:X&edpv,b? rPDACszAS-nkNch-Nryw:$jupUj\\n T'PG&GFGP !&:aLjKL$u.qrCpIadhkIXtRBEtnxE:cTmFXpOPq&aZ!Q\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_x = torch.tensor(stoi[\"\\n\"]).reshape((1,1))\n",
    "new_x = m.generate(initial_x, n=100)\n",
    "decode(new_x[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c08f6e-1821-46e0-a0da-f7de9d6f48fe",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ae99a6c-8ad8-4a88-9a73-a50391bb8f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': 4.202554225921631, 'test_loss': 4.200745582580566}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_model(m: BGLM) -> dict:\n",
    "    return dict(\n",
    "        train_loss=avg_loss(m, train_data),\n",
    "        test_loss=avg_loss(m, test_data)\n",
    "    )\n",
    "\n",
    "@torch.no_grad()\n",
    "def avg_loss(m: BGLM, data: Tensor) -> float:\n",
    "    m.eval()\n",
    "    n_batches = 200\n",
    "    loss = torch.tensor([_loss(m, data) for _ in range(n_batches)]).mean()\n",
    "    m.train()\n",
    "    return loss.tolist()\n",
    "\n",
    "def _loss(m: BGLM, data: Tensor) -> float:\n",
    "    xs, ys = get_batch(data)\n",
    "    _, loss = m(xs, ys)\n",
    "    return loss\n",
    "\n",
    "evaluate_model(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348349c7-7e12-42d0-9968-9c1c3187d6c2",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28a5ed1e-d5fd-4118-81ce-eba1754241a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': 4.188299179077148, 'test_loss': 4.18881368637085}\n",
      "{'train_loss': 2.5251500606536865, 'test_loss': 2.534402847290039}\n",
      "{'train_loss': 2.4450864791870117, 'test_loss': 2.453768491744995}\n",
      "{'train_loss': 2.4123029708862305, 'test_loss': 2.428366184234619}\n",
      "{'train_loss': 2.3954243659973145, 'test_loss': 2.409773349761963}\n",
      "{'train_loss': 2.402365207672119, 'test_loss': 2.4012603759765625}\n",
      "{'train_loss': 2.3836829662323, 'test_loss': 2.4104301929473877}\n",
      "{'train_loss': 2.377206325531006, 'test_loss': 2.390448808670044}\n",
      "{'train_loss': 2.375992774963379, 'test_loss': 2.378732204437256}\n",
      "{'train_loss': 2.3648550510406494, 'test_loss': 2.38360857963562}\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 32\n",
    "n_iters = 10000\n",
    "for idx in range(n_iters):\n",
    "    xs_, ys_ = get_batch(train_data)\n",
    "    logits, loss = m(xs_, ys_)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if idx % (n_iters / 10) == 0:\n",
    "        print(evaluate_model(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afbb04d4-2ac8-4d47-8ea8-329bb93b3925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tisheassy, save, ome int fr, aspoef\n",
      "Fo'lsam illvestimy Polod easene atu ly avent, sosu coly, sba sha, llafr ly riothe:\n",
      "Mand hear ad\n",
      "Mel, thal anderd tw bough, sheasul,\n",
      "Af a blu. Shon:\n",
      "Merd;\n",
      "Whanchour gren alt keasn he cagtern momousle,\n",
      "Loll chisu ghrer ame mesend tind be se thas glok kee.\n",
      "\n",
      "OO,\n",
      "Porf toen ithis sourst. I ske, stawnd; dso yory fa tad;-\n",
      "Ber ticontt amon Edingeanch mud ingo.\n",
      "\n",
      "\n",
      "J'oniceaced by, hourarson to do--\n",
      "An meand thare tn yo sous, flen ack hre'nt whee, Min hed san dyo es othe m\n"
     ]
    }
   ],
   "source": [
    "new_x = m.generate(initial_x, n=500)\n",
    "print(decode(new_x[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
