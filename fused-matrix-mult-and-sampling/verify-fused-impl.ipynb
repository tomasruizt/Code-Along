{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d67f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72853041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7383e0a5",
   "metadata": {},
   "source": [
    "# Gumbel-Max Trick\n",
    "Sampling from the mulitnomial is equivalent to taking the argmax over logits plus standard Gumbel noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e38e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logits = torch.tensor([2.5, 2, 3])\n",
    "probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "\n",
    "def gumbel_sample(logits: torch.Tensor, n: int):\n",
    "    gumbel_noise = -(-torch.rand((n, len(logits))).log()).log()\n",
    "    return torch.argmax(logits + gumbel_noise, dim=-1)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "ax = sns.barplot(probs.cpu(), ax=axs[0])\n",
    "ax.grid(axis=\"y\")\n",
    "ax.set_title(\"Barplot of probs\")\n",
    "\n",
    "gumbel_samples = gumbel_sample(logits, n=1000)\n",
    "ax = sns.histplot(gumbel_samples.cpu(), stat=\"probability\", ax=axs[1], binwidth=0.4)\n",
    "ax.grid(axis=\"y\")\n",
    "ax.set_title(\"Histogram of gumbel samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79aa3b9",
   "metadata": {},
   "source": [
    "# Fused MM-Sample\n",
    "We now attempt to sample from the logits without materializing them.\n",
    "We compute the logits incrementally, and as we do, we keep track of the gumbel max index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9534dc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100  # V\n",
    "hidden_size = 10  # D\n",
    "logits1 = torch.arange(-vocab_size / 2, vocab_size / 2)[None, :]  # [1, V]\n",
    "logits2 = torch.arange(vocab_size / 2, -vocab_size / 2, step=-1)[None, :]  # [1, V]\n",
    "logits = torch.cat([logits1, logits2], dim=0)  # [seq_len, V]\n",
    "hl_seq_len = logits.shape[0]\n",
    "# use SVD to construct the hidden states that yield the logits\n",
    "# use pseudoinverse to construct the weights.\n",
    "# (there are many ways to do this, this is just one)\n",
    "# W @ H = L.T\n",
    "#  -> W = L.T @ H⁻¹\n",
    "U, S, Vt = torch.linalg.svd(logits, full_matrices=False)\n",
    "hidden_states = torch.cat(  # [D, seq_len]\n",
    "    [\n",
    "        U.T,\n",
    "        torch.rand((hidden_size - hl_seq_len, hl_seq_len)),  # padding\n",
    "    ],\n",
    ")\n",
    "weights = logits.T @ torch.linalg.pinv(hidden_states)  # [V, D]\n",
    "assert torch.allclose(weights @ hidden_states, logits.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fa0eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To bfloat 16\n",
    "weights = weights.bfloat16()\n",
    "hidden_states = hidden_states.bfloat16()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68245803",
   "metadata": {},
   "source": [
    "## Baseline: PyTorch Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee03179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fused_mm_sampling import sample\n",
    "\n",
    "\n",
    "def plot_samples(samples: torch.Tensor, seq_len: int, num_samples: int):\n",
    "    data = {\n",
    "        \"sample\": samples.flatten().cpu(),\n",
    "        \"seq\": [seq for seq in range(seq_len) for _ in range(num_samples)],\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    ax = sns.histplot(df, x=\"sample\", hue=\"seq\", bins=100)\n",
    "    ax.grid(axis=\"y\", alpha=0.5)\n",
    "\n",
    "\n",
    "num_samples = 1000\n",
    "temperature = 5\n",
    "samples, probs = sample(\n",
    "    weights,\n",
    "    hidden_states,\n",
    "    num_samples=num_samples,\n",
    "    temperature=temperature,\n",
    "    return_probs=True,\n",
    ")  # [seq_len, num_samples]\n",
    "plot_samples(samples, hl_seq_len, num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c477ef34",
   "metadata": {},
   "source": [
    "## Fused PyTorch Incremental Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad0107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fused_mm_sampling import incremental_sample_pt\n",
    "\n",
    "samples2 = incremental_sample_pt(weights, hidden_states, num_samples, temperature)\n",
    "plot_samples(samples2, hl_seq_len, num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a418a8c0",
   "metadata": {},
   "source": [
    "# Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcebec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fused_mm_sampling import fused_sample_triton\n",
    "\n",
    "samples3 = fused_sample_triton(\n",
    "    weights,\n",
    "    hidden_states,\n",
    "    num_samples,\n",
    "    temperature,  # temperature,\n",
    "    seed=111,\n",
    ")\n",
    "plot_samples(samples3, hl_seq_len, num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc472899",
   "metadata": {},
   "source": [
    "# Compare Speed - Realistic Example\n",
    "Now we test a realistic example with a large `vocab_size=256k`, and a large `hidden_size=5120`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b85a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 256_000\n",
    "hidden_size = 5120\n",
    "seq_len = 256\n",
    "num_samples = 1\n",
    "speedtest_kwargs = dict(\n",
    "    hidden_states=torch.randn((hidden_size, seq_len)).bfloat16(),\n",
    "    weights=torch.randn((vocab_size, hidden_size)).bfloat16(),\n",
    "    num_samples=num_samples,\n",
    "    temperature=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db41f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_compiled = torch.compile(sample)\n",
    "_ = sample_compiled(**speedtest_kwargs)\n",
    "# sample_incremental_pt_compiled = torch.compile(incremental_sample_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ee6d07",
   "metadata": {},
   "source": [
    "%timeit fused_sample_triton(**speedtest_kwargs, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7533ce",
   "metadata": {},
   "source": [
    "%timeit sample_compiled(**speedtest_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700d7ce0",
   "metadata": {},
   "source": [
    "# Memory Profiling\n",
    "Should be observable with the PyTorch Memory Timeline: https://pytorch.org/blog/understanding-gpu-memory-1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b5f935",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_numel = vocab_size * 256  # new_seqlen\n",
    "logits_bytes = logits_numel * 2  # bfloat16\n",
    "logits_gb = logits_bytes / 10**9\n",
    "print(f\"logits_numel: {logits_numel:,}\")\n",
    "print(f\"logits_gb: {logits_gb:.2f} GB\")\n",
    "\n",
    "weights_numel = vocab_size * hidden_size\n",
    "weights_bytes = weights_numel * 2  # bfloat16\n",
    "weights_gb = weights_bytes / 10**9\n",
    "print(f\"weights_numel: {weights_numel:,}\")\n",
    "print(f\"weights_gb: {weights_gb:.2f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
