{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72853041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "torch.set_default_device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e38e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.tensor([2.5, 2, 3])\n",
    "probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "print(probs)\n",
    "ax = sns.barplot(probs.cpu())\n",
    "ax.grid(axis=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66048127",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = torch.multinomial(probs, num_samples=1000, replacement=True)\n",
    "print(torch.bincount(samples) / len(samples))\n",
    "ax = sns.histplot(samples.cpu(), stat=\"probability\")\n",
    "ax.grid(axis=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae65d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_logits = torch.tensor([3.0, 2.0, 1.0])\n",
    "spec_probs = torch.nn.functional.softmax(spec_logits, dim=-1)\n",
    "spec_samples = torch.multinomial(spec_probs, num_samples=1000, replacement=True)\n",
    "print(spec_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c000c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.histplot(spec_samples.cpu(), stat=\"probability\")\n",
    "ax.grid(axis=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1f79ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rejection_sample(probs: torch.Tensor, spec_probs: torch.Tensor, idx: torch.Tensor):\n",
    "#     \"\"\"\n",
    "#     Target model distribution: q(x)\n",
    "#     Draft model distribution: p(x)\n",
    "#     \"\"\"\n",
    "#     q = probs[idx]\n",
    "#     p = spec_probs[idx]\n",
    "#     r = torch.rand(1)\n",
    "#     if r < torch.clamp(q / p, max=1.0):\n",
    "#         return idx\n",
    "#     new_p = torch.clamp(probs - spec_probs, min=0.0)\n",
    "#     return torch.multinomial(new_p, num_samples=1, replacement=True)[0]\n",
    "\n",
    "\n",
    "def rejection_sample(probs: torch.Tensor, spec_probs: torch.Tensor, idxs: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Target model distribution: q(x)\n",
    "    Draft model distribution: p(x)\n",
    "    Vectorized implementation\n",
    "    \"\"\"\n",
    "    qs = probs[idxs]\n",
    "    ps = spec_probs[idxs]\n",
    "    rs = torch.rand(len(idxs))\n",
    "    keep_mask = rs < torch.clamp(qs / ps, max=1.0)\n",
    "    new_p = torch.clamp(probs - spec_probs, min=0.0)  # pseudo-probability\n",
    "    new_samples = torch.multinomial(new_p, num_samples=len(idxs), replacement=True)\n",
    "    return torch.where(keep_mask, idxs, new_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0df706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rej_samples = torch.tensor([rejection_sample(probs, spec_probs, x) for x in spec_samples])\n",
    "rej_samples = rejection_sample(probs, spec_probs, spec_samples)\n",
    "ax = sns.histplot(rej_samples.cpu(), stat=\"probability\")\n",
    "ax.grid(axis=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7383e0a5",
   "metadata": {},
   "source": [
    "# Gumbel-Max Trick\n",
    "Sampling from the mulitnomial is equivalent to taking the argmax over logits plus standard Gumbel noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fab93e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_sample(logits: torch.Tensor, n: int):\n",
    "    gumbel_noise = -(-torch.rand((n, len(logits))).log()).log()\n",
    "    return torch.argmax(logits + gumbel_noise, dim=-1)\n",
    "\n",
    "\n",
    "gumbel_samples = gumbel_sample(logits, n=1000)\n",
    "ax = sns.histplot(gumbel_samples.cpu(), stat=\"probability\")\n",
    "ax.grid(axis=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79aa3b9",
   "metadata": {},
   "source": [
    "# Fused MM-Sample\n",
    "We now attempt to sample from the logits without materializing them.\n",
    "We compute the logits incrementally, and as we do, we keep track of the gumbel max index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9534dc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100  # V\n",
    "hidden_size = 10  # D\n",
    "logits1 = torch.arange(-vocab_size / 2, vocab_size / 2)[None, :]  # [1, V]\n",
    "logits2 = torch.arange(vocab_size / 2, -vocab_size / 2, step=-1)[None, :]  # [1, V]\n",
    "logits = torch.cat([logits1, logits2], dim=0)  # [seq_len, V]\n",
    "seq_len = logits.shape[0]\n",
    "# use SVD to construct the hidden states that yield the logits\n",
    "# use pseudoinverse to construct the weights.\n",
    "# (there are many ways to do this, this is just one)\n",
    "# W @ H = L.T\n",
    "#  -> W = L.T @ H⁻¹\n",
    "U, S, Vt = torch.linalg.svd(logits, full_matrices=False)\n",
    "hidden_states = torch.cat(  # [D, seq_len]\n",
    "    [\n",
    "        U.T,\n",
    "        torch.rand((hidden_size - seq_len, seq_len)),  # padding\n",
    "    ],\n",
    ")\n",
    "weights = logits.T @ torch.linalg.pinv(hidden_states)  # [V, D]\n",
    "assert torch.allclose(weights @ hidden_states, logits.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68245803",
   "metadata": {},
   "source": [
    "## Baseline: PyTorch Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee03179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(\n",
    "    weights: torch.Tensor,\n",
    "    hidden_states: torch.Tensor,\n",
    "    num_samples: int,\n",
    "    temperature: float,\n",
    "):\n",
    "    logits = weights @ hidden_states  # [seq_len, V]\n",
    "    logits -= torch.max(logits, dim=0, keepdim=True).values\n",
    "    probs = torch.nn.functional.softmax(logits / temperature, dim=0)  # [seq_len, V]\n",
    "    samples = torch.multinomial(probs.T, num_samples=num_samples, replacement=True)\n",
    "    return samples, probs\n",
    "\n",
    "def plot_samples(samples: torch.Tensor, seq_len: int, num_samples: int):\n",
    "    data = {\n",
    "        \"sample\": samples.flatten().cpu(),\n",
    "        \"seq\": [seq for seq in range(seq_len) for _ in range(num_samples)],\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    ax = sns.histplot(df, x=\"sample\", hue=\"seq\", bins=100)\n",
    "    ax.grid(axis=\"y\", alpha=0.5)\n",
    "\n",
    "\n",
    "num_samples = 1000\n",
    "temperature = 5\n",
    "samples, probs = sample(\n",
    "    weights, hidden_states, num_samples=num_samples, temperature=temperature\n",
    ")  # [seq_len, num_samples]\n",
    "plot_samples(samples, seq_len, num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c477ef34",
   "metadata": {},
   "source": [
    "## Fused PyTorch Incremental Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e098bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def incremental_sample_pt(\n",
    "    weights: torch.Tensor,\n",
    "    hidden_states: torch.Tensor,\n",
    "    num_samples: int,\n",
    "    temperature: float,\n",
    "):\n",
    "    V, D = weights.shape\n",
    "    D, seq_len = hidden_states.shape\n",
    "    block_size = 8\n",
    "    # compute logits blocks\n",
    "    gumbel_max = float(\"-inf\") * torch.ones(size=(num_samples, seq_len))\n",
    "    gumbel_max_idx = torch.empty(size=(num_samples, seq_len), dtype=torch.long)\n",
    "    n_blocks = torch.ceil(torch.tensor(V) / block_size).int()\n",
    "    for blk_idx in range(n_blocks):\n",
    "        idx_from = blk_idx * block_size\n",
    "        idx_to = (blk_idx + 1) * block_size\n",
    "        w_blk = weights[idx_from:idx_to]  # [block_size, D]\n",
    "        logits_blk = w_blk @ hidden_states / temperature  # [seq_len, block_size]\n",
    "        unif_noise = torch.rand((num_samples, *logits_blk.shape))\n",
    "        gumbel_noise = -(-unif_noise.log()).log()\n",
    "        new_max, new_max_idx_local = torch.max(logits_blk + gumbel_noise, dim=1)\n",
    "        new_max_idx_global = idx_from + new_max_idx_local\n",
    "\n",
    "        replace_mask = new_max > gumbel_max\n",
    "        gumbel_max[replace_mask] = new_max[replace_mask]\n",
    "        gumbel_max_idx[replace_mask] = new_max_idx_global[replace_mask]\n",
    "    return gumbel_max_idx.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad0107",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples2 = incremental_sample_pt(weights, hidden_states, num_samples, temperature)\n",
    "plot_samples(samples2, seq_len, num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a418a8c0",
   "metadata": {},
   "source": [
    "# Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1546cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.environ[\"TRITON_INTERPRET\"] = \"1\"\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "\n",
    "def fused_sample_triton(\n",
    "    weights: torch.Tensor,\n",
    "    hidden_states: torch.Tensor,\n",
    "    num_samples: int,\n",
    "    temperature: float,\n",
    "    seed: int,\n",
    "):\n",
    "    V, D = weights.shape\n",
    "    D, seq_len = hidden_states.shape\n",
    "    block_size_v = 8\n",
    "    block_size_d = triton.next_power_of_2(D)\n",
    "    grid_size = triton.cdiv(V, block_size_v)\n",
    "\n",
    "    maxs = torch.zeros((grid_size, seq_len, num_samples), dtype=torch.float32)\n",
    "    maxs_idx = torch.zeros_like(maxs, dtype=torch.long)\n",
    "\n",
    "    # def grid(meta):\n",
    "    #     return (triton.cdiv(V, meta[\"BLOCK_SIZE\"]),)\n",
    "\n",
    "    seqlen_p2 = triton.next_power_of_2(seq_len)\n",
    "    num_samples_p2 = triton.next_power_of_2(num_samples)\n",
    "    noise_size = block_size_v * seqlen_p2 * num_samples_p2\n",
    "\n",
    "    grid = (grid_size,)\n",
    "\n",
    "    fused_sample_triton_kernel[grid](\n",
    "        weights_ptr=weights,\n",
    "        hidden_states_ptr=hidden_states,\n",
    "        max_out_ptr=maxs,\n",
    "        max_out_idx_ptr=maxs_idx,\n",
    "        vocab_size=V,\n",
    "        hidden_size=D,\n",
    "        seq_len=seq_len,\n",
    "        num_samples=num_samples,\n",
    "        temperature=temperature,\n",
    "        seed=seed,\n",
    "        BLOCK_SIZE_V=block_size_v,\n",
    "        BLOCK_SIZE_D=block_size_d,\n",
    "        noise_size=noise_size,\n",
    "        num_samples_p2=num_samples_p2,\n",
    "        seqlen_p2=seqlen_p2,\n",
    "    )\n",
    "\n",
    "    # 2nd stage: reduction\n",
    "    idxs = maxs.max(axis=0).indices\n",
    "    samples = maxs_idx.gather(dim=0, index=idxs[None, :])\n",
    "    return samples.squeeze(0)  # [seq_len, num_samples]\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def fused_sample_triton_kernel(\n",
    "    weights_ptr,\n",
    "    hidden_states_ptr,\n",
    "    max_out_ptr,  # [grid_size, seq_len, num_samples]\n",
    "    max_out_idx_ptr,  # [grid_size, seq_len, num_samples]\n",
    "    vocab_size,  # V\n",
    "    hidden_size: tl.constexpr,  # D\n",
    "    seq_len: tl.constexpr,\n",
    "    num_samples: tl.constexpr,\n",
    "    temperature: float,  # should this be a tl.constexpr?\n",
    "    seed: int,\n",
    "    BLOCK_SIZE_V: tl.constexpr,\n",
    "    BLOCK_SIZE_D: tl.constexpr,\n",
    "    noise_size: tl.constexpr,\n",
    "    num_samples_p2: tl.constexpr,\n",
    "    seqlen_p2: tl.constexpr,\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    block_start = pid * BLOCK_SIZE_V\n",
    "\n",
    "    # We don't instantiate gumbel_max yet, because each program just writes\n",
    "    # its local max into main memory for a parallel reduction in stage 2.\n",
    "\n",
    "    offsets_v = block_start + tl.arange(0, BLOCK_SIZE_V)\n",
    "    offsets_h = tl.arange(0, BLOCK_SIZE_D)\n",
    "    mask_v = offsets_v < vocab_size\n",
    "    mask_h = offsets_h < hidden_size\n",
    "\n",
    "    w_offsets = offsets_v[:, None] * hidden_size + offsets_h[None, :]\n",
    "    w_blk = tl.load(\n",
    "        weights_ptr + w_offsets,\n",
    "        mask=mask_v[:, None] & mask_h[None, :],\n",
    "    )\n",
    "\n",
    "    offset_seqlen = tl.arange(0, seq_len)\n",
    "    hidden_states_blk = tl.load(\n",
    "        hidden_states_ptr + offset_seqlen[None, :] + seq_len * offsets_h[:, None],\n",
    "        mask=mask_h[:, None],\n",
    "    )\n",
    "    logits_blk = tl.dot(w_blk, hidden_states_blk) / temperature  # [Vblk, seq_len]\n",
    "\n",
    "    # Note: Creating appropriately sized tensors is tricky because\n",
    "    # tl.arange() only accepts tl.constexpr that are powers of 2.\n",
    "    noise_offsets = tl.arange(0, noise_size).reshape(\n",
    "        (num_samples_p2, BLOCK_SIZE_V, seqlen_p2)\n",
    "    )\n",
    "    # Note: Each program needs a different seed, otherwise they\n",
    "    # all create the same noise, leading to sampling artifacts.\n",
    "    unif_noise = tl.rand(seed + pid, noise_offsets)\n",
    "    gumbel_noise = -tl.log(-tl.log(unif_noise))\n",
    "\n",
    "    gumbel_max, gumbel_max_idx_local = tl.max(\n",
    "        logits_blk + gumbel_noise, axis=1, return_indices=True\n",
    "    )  # [num_samples_p2, seqlen_p2]\n",
    "    gumbel_max_idx_global = gumbel_max_idx_local + block_start\n",
    "\n",
    "    out_blk_start = pid * seq_len * num_samples\n",
    "\n",
    "    # Note: It makes a difference if indices are row-major or column-major\n",
    "    # Note: The stride needs to match the non-padded shape!\n",
    "    out_offsets = (\n",
    "        tl.arange(0, num_samples_p2)[:, None]\n",
    "        + num_samples * tl.arange(0, seqlen_p2)[None, :]\n",
    "    )\n",
    "    out_mask = num_samples > tl.arange(0, num_samples_p2)[:, None]\n",
    "    tl.store(\n",
    "        max_out_ptr + out_blk_start + out_offsets,\n",
    "        gumbel_max,\n",
    "        mask=out_mask,\n",
    "    )\n",
    "    tl.store(\n",
    "        max_out_idx_ptr + out_blk_start + out_offsets,\n",
    "        gumbel_max_idx_global,\n",
    "        mask=out_mask,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcebec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "samples3 = fused_sample_triton(\n",
    "    weights,\n",
    "    hidden_states,\n",
    "    num_samples,\n",
    "    temperature,  # temperature,\n",
    "    seed=111,\n",
    ")\n",
    "plot_samples(samples3, seq_len, num_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36aee8a",
   "metadata": {},
   "source": [
    "# Compare Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db41f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_compiled = torch.compile(sample)\n",
    "sample_incremental_pt_compiled = torch.compile(incremental_sample_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1652fbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit sample(weights, hidden_states, num_samples, temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ceb637",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit sample_compiled(weights, hidden_states, num_samples, temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7356f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit incremental_sample_pt(weights, hidden_states, num_samples, temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c81e7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit fused_sample_triton(weights, hidden_states, num_samples, temperature, seed=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
